<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Keyword Extraction</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <div class="top_area"><div class="top_title"><h1> KEYWORD EXTRACTION FOR WEBPAGES </h1></div>
    
    <div class="top_content">
      <h3>Machine Learning Group School of Computing University of Eastern Finland </h3> 
      <h4>Author Himat Shah (2018-2022)</h4>
      </br>
      </div>
      </div>
    
    <header>
      <nav>
      
        <ul>
          <li><a href="#Introduction">Introduction</a></li>
          <li><a href="#Hrank">HRANK</a></li>
          <li><a href="#Drank">DRANK</a></li>
          <li><a href="#Webrank">WebRank</a></li>
          <li><a href="#Acirank">ACI-Rank</a></li>
        </ul>
      </nav>
    </header>

    <main>
      <section id="Introduction">
        <div class="container">
          <div class="title">
            <h2>About Keyword Extraction</h2>
            <article>
              <p>
                Google defnes a keyword as an isolated word or phrase that
                provides concise high-level information about content to readers
                . With the increasing amount of data, users need more resources
                and time to understand content. Keywords make it easier to
                understand the meaning of a text in fewer words.
              </p>
              <p>
                In short, keywords summarize the key points presented in the
                text. When searching for information on search engines, keywords
                play a signifcant role in fnding relevant content. Keywords are
                the mostinformative part of a text; they are the most prominent
                words in the text and describe its content. Keywords are
                necessary in situations involving huge amounts of text data that
                need to be processed automatically. Keywords are widely used in
                document summarization, indexing, categorization, and clustering
                of huge datasets. Many scientifc publications contain keyword
                lists that have been explicitly assigned by their authors. Other
                documents, however, have not been assigned keywords . As
                webpages are constantly updated, it is difcult to create
                keywords manually. Manual keyword assignment is labor intensive,
                time consuming, and error prone.
              </p>
              <p>
                Specialized curators use fxed taxonomies for manual keyword
                generation, but in some cases, the keywords chosen by the author
                are not sufciently comprehensive and accurate. Without
                high-quality keywords, users fail to catch relevant information
                . Keywords ofer readers a concise high-level summary of a
                documents content, thereby improving their understanding of that
                text. Keywords are the most relevant and important indicator for
                users seeking to grasp the fundamentals of a topic when scanning
                or skimming an article. Keyword extraction is a basic step in
                many text-mining and natural language processing (NLP)
                techniques, including text summarization, information retrieval,
                topic modeling, clustering, and content-based advertisement
                systems. Finding the relevant webpages, a user is seeking is
                often a challenging task for which representative keywords or
                keyphrases.
              </p>

              <p>
                This article addresses the issue of automatic extracting
                keywords from webpages. The majority of existing keyword
                extraction methods use language-dependent Natural Language
                Processing (NLP) based techniques, including Part-of-Speech
                (POS) tagging, stemming, and lemmatization, which makes it
                complex to generalize a method for different languages. The main
                purpose of the research is to extract only those
                language-independent features of web pages in order to find a
                method that can be applied in different languages.
              </p>
              <p>
                So far, studies on language-independent approaches have been
                limited because they usually perform worse than methods that
                take advantage of linguistic features. Extracting keywords from
                web documents involves two main challenges: the first is the
                presence of noise and irrelevant data such as navigation bars,
                menus, comments and advertisements and the second consists in
                the presence of multiple topics and multiple languages.
                Therefore, it is very important to have a general keyword
                extraction method that can extract keywords without relying on
                any specific language.
              </p>

              <p>
                addresses the challenges of keyword extraction by developing and
                testing four new techniques, both language-dependent and
                language-independent as well as supervised and unsupervised.
                Special attention is paid to finding the most relevant features
                for identifying good keyword and keyphrase candidates. The work
                deals with statistical, linguistic, and structural features as
                well as their combinations. This diversity of approaches serves
                the pragmatic overall goal of finding the best available methods
                by assessing the relative performances of the newly developed as
                well as existing methods on a number of different datasets.
              </p>

              <p>
                For this purpose the author proposes four new automatic keyword
                extraction methods for webpages: Hrank, D-rank, WebRank, and
                ACI-rank
              </p>
            </article>
          </div>
        </div>
      </section>

      <section id="Hrank">
        <div class="container">
          <div class="title">
            <h2>HRANK</h2>
            <article>
              <p>
                In this Hrank section we  we will discuss four main parts:<br/>
                 (1) Introduction (2) Methodology:
                 (3) Python Implementation (4) Output Section 
             
              </p>
              <div class="title"><h3> (1) Introduction </h3></div>
              <p>
                We study the importance of the distribution of semantically similar POS tags, such as nouns, adjectives, and verbs in the extraction of relevant keywords from the web page
              </p><p><li>	A new keywords extraction method that requires a minimum knowledge of DOM structure. </li>
                <li>A simple measure TF performs better than the more complex methods. </li>
                  <li>However, the combination of nouns, adjectives, and verbs improves performance when TF fails.</li>
                 
<li> A new keywords extraction method that requires a minimum knowledge of DOM structure. </li>
<li>The proposed method outperforms CL-Rank, TextRank, and TF. A simple measure TF performs better than the more complex methods. However, the combination of nouns, adjectives, and verbs improves performance when TF fails.</li>
</p>               
                

                
              
            </article>
          </div>
          <div class="methodology">
            <div class="title"><h3> (2) Methodology </h3></div>
            <article>
             
              <p>Fig. 1. presents the workflow of the proposed keywords extraction method. The method has two modules: (1) pre-processing and (2) keyword extraction. The pre-processing module involves the extraction of the natural language text from the web page. The keyword extraction module utilizes the text from the pre-processing module. 
                In the pre-processing module, the first three functions involve the filtering of the text from all the other content of a web page. All the content of a web page is extracted using a document object model (DOM) and X-path function. The text that belongs to javascript scripting language and cascade style sheets is eliminated in the text filtering function. The special characters, such as @,*,Â£, or $, punctuation marks, and numbers are also filtered out using the regular expression in the text filtering function. Similarly, the text filtering function also involves the removal of the stop words from the text. The stop words are the natural language words that have minimal or no meaning, such as and, the, a, and an. The filtered text can now be utilized for natural language processing.
                The POS extractor, normalize text, and separate POS functions involve the natural language processing on the filtered text. The POS extractor function divides the text into tokens. A token is a whitespace-separated unit of text. The tokens are assigned the POS tags, such as nouns, adjectives, and verbs. 
                <p>  <img
                  src="./images/Hrank_workflow.PNG"
                  alt="Dranks"
                  width="600"
                  height="300"
                />
                <p>Fig. 1. Workflow of Drank</p></p>
                <p>The tokens with POS tags are further normalized.
                 The normalization is the process of replacing the inflected forms of a word with the root word. The inflected form represents the different usage of a word in the sentences. For example, finds, finding, and found are the inflected forms of the word find. An inflected form of a word has a changed spelling or ending. In natural language processing, the lemmatization is used to find the inflected form of the words with different spellings, such as finds and found for the word find in the above example. Unlike lemmatization, the stemming process takes care of the prefixes and suffixes to find the root word, such as finding in the abovementioned example. The output of the normalization process is the tokens with all the inflected forms replaced with their root word.
                The lists of the POS-tagged tokens are provided to the separate POS function, which separates the tokens into the lists of nouns, adjectives, and verbs. The lists are provided to the count frequency function. The count frequency function calculates the frequency of the words in the separate lists having nouns, adjectives, and verbs. The top-frequent tokens are selected as candidate keywords. The semantically similar words among top-frequent tokens are grouped together using a lexical database, named as WordNet. The lexical database helps in finding the synsets of the words. The synset is a set of one or more synonyms that can be used interchangeably in some context [20].                                                                                                                                                                                                                                                                                                                                                                                                                            
              </p>
              <p>
              We compute the semantic similarity of two different words using path-similarity, which is based on the WordNet [21]. The words that have no synonyms in the WordNet are removed from the lists. The path-similarity metric calculates the score between two different words in terms of their relatedness. We use path-similarity because it is very simple and it operates based on a parent-child relationship like a tree. Therefore, it is more convenient to use in our case.
                Three similarity matrices are created independently for the nouns, adjectives, and verbs. The similarity matrices are utilized in clustering the related words. We use an agglomerative clustering to find similar words in the lists.  The clusters are scored by counting the frequencies of all the words in each cluster. The clusters are ranked according to the scores.
                </p>  
            </article>
            <div class="title"><h3> (3) Python Implementation</h3></div>
            <p><ol>
             <li>Extract Text</li>
             <li>Preprocess Text</li>
             <li>POS Tags Seperation</li>
             <li>Word Net semantic similarity</li>
             <li>Cluster Words</li>
             <li>Keyword Ranking and Selection</li>
           </ol></p>
          </div>
          <div class="coding" >
   
            <pre><code>
              
              <h4> Import packages</h4>
              
              <code> Imports
                import urllib
                import nltk
                import sys
                import re 
                
                import lxml
                import math
                import string
                import textwrap
                import requests
                
                from nltk.corpus import stopwords
                from bs4 import BeautifulSoup
                from nltk import word_tokenize
                from nltk.stem import WordNetLemmatizer
                from collections import defaultdict,Counter
                from nltk.corpus import stopwords
                from collections import defaultdict 
                from bs4.element import Comment
                
                from nltk import wordpunct_tokenize
                from urllib.parse import urlparse 
                
                import pandas as pd 
                import numpy as np
                
                Common_Nouns ="january debt est dec big than who use jun jan feb mar apr may jul agust dec oct nov sep dec  product continue second secodns".split(" ")
                URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
                URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
                UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
                <hr class ="new3"/>
        <h4>(1) Extract Text of Webpage</h4>
       
                
                def Scrapper1(element):
                    if element.parent.name in [UselessTagsText]:
                        return False
                    if isinstance(element, Comment):
                        return False
                    return True
                
                def Scrapper2(body):             
                    soup = BeautifulSoup(body, 'lxml')      
                    texts = soup.findAll(text=True)   
                    name =soup.findAll(name=True) 
                    visible_texts = filter(Scrapper1,texts)        
                    return u" ".join(t.strip() for t in visible_texts)
                
                def Scrapper3(text):                  
                    lines = (line.strip() for line in text.splitlines())    
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    return u'\n'.join(chunk for chunk in chunks if chunk)
                
                
                def Scrapper_title_4(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()
                  title=[]
                  
                  soup = BeautifulSoup(html, 'lxml') 
                  title.append(soup.title.string)
                  return(title,urls)
                
                def Web_Funtion(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()  
                  Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
                 
                  raw =Scrapper2(html)
                  Raw_text = Scrapper3(raw) 
                  return(Raw_text,Raw_HTML_Soup)  
                
              
                
                    </code></pre>
          </div>
          <div class="output">
            <div class="result">
              <h4></h4>
              <pre><code></code></pre>
            </div>
          </div>
        </div>
      </section>

      <section id="Drank">
        <div class="container">
          <div class="title">
            <h2>Drank</h2>
            <article>
              
                <p>
                  Drank section comprises of four sections:<br/>
                   (1) Introduction (2) Methodology:
                   (3) Python Implementation (4) Output Section 
               
                </p>
                <div class="title"><h3> (1) Introduction </h3></div>
              <p>
                Work deals with webpage keyword extraction, which is crucial for the
      information retrieval task performed by search engines browsing
      through the internet. As such, keyword extraction is a specific kind
      of information extraction task, where the use of a natural language,
      or even several languages, poses severe challenges. To conquer these
      challenges, appropriate natural language processing (NLP) techniques
      have to be applied. As the method is dealing with webpages, the task
      is further complicated by the varying structure and layout of the
      pages. Even if Google search is widely and successfully used by a
      vast number of people for all so many purposes, the search results
      are often far from optimal, and processing natural language
      documents remain challenging.
    </p>
    <p>
      The D-rank method is an unsupervised method where the candidate
      keywords were ranked based on their position in the content after
      extracting their features from the DOM structure. The author tested
      the proposed method on a dataset of webpages in three languages:
      English, Finnish, and German.
    </p>             
                

                
              
            </article>
          </div>
          <div class="methodology">
            <div class="title"><h3> (2) Methodology </h3></div>
            <article>
              <p>
              <img
                  src="./images/Drank_workflow.PNG"
                  alt="Dranks"
                  width="600"
                  height="300"/>
                </p>
            </article>
           
             <div class="title"><h3> (3) Python Implementation</h3></div>
            <p><ol>
             <li>Extract Text</li>
             <li>Preprocess Text</li>
             <li>POS Tags Seperation</li>
             <li>Word Net semantic similarity</li>
             <li>Cluster Words</li>
             <li>Keyword Ranking and Selection</li>
           </ol></p>
          </div>
          <div class="coding" >
   
            <pre><code>
              <hr class ="new3"/>
              #Imports
             
             import urllib
             import nltk
             import sys
             import re 
             
             import lxml
             import math
             import string
             import textwrap
             import requests
             
             from nltk.corpus import stopwords
             from bs4 import BeautifulSoup
             from nltk import word_tokenize
             from nltk.stem import WordNetLemmatizer
             from collections import defaultdict,Counter
             from nltk.corpus import stopwords
             from collections import defaultdict 
             from bs4.element import Comment
             
             from nltk import wordpunct_tokenize
             from urllib.parse import urlparse 
             
             import pandas as pd 
             import numpy as np
             
             import warnings
             warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
             
             Common_Nouns ="january debt est dec big than who use jun jan feb mar apr may jul agust dec oct nov sep dec  product continue one two three four fi".split(" ")
             URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
             URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
             UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
             
             <hr class ="new3"/>
              1. Extract Text of webpage
             
             
             def Scrapper1(element):
                 if element.parent.name in [UselessTagsText]:
                     return False
                 if isinstance(element, Comment):
                     return False
                 return True
             
             def Scrapper2(body):             
                 soup = BeautifulSoup(body, 'lxml')      
                 texts = soup.findAll(text=True)   
                 name =soup.findAll(name=True) 
                 visible_texts = filter(Scrapper1,texts)        
                 return u" ".join(t.strip() for t in visible_texts)
             
             def Scrapper3(text):                  
                 lines = (line.strip() for line in text.splitlines())    
                 chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                 return u'\n'.join(chunk for chunk in chunks if chunk)
             
             
             def Scrapper_title_4(URL):
               req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
               con = urllib.request.urlopen(req)
               html= con.read()
               title=[]
               
               soup = BeautifulSoup(html, 'lxml') 
               title.append(soup.title.string)
               return(title,urls)
             
             def Web_Funtion(URL):
               req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
               con = urllib.request.urlopen(req)
               html= con.read()  
               Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
              
               raw =Scrapper2(html)
               Raw_text = Scrapper3(raw) 
               return(Raw_text,Raw_HTML_Soup) 
               <hr class ="new3"/>
              (2) Language Detection of Webpage Text
             
             
             def _calculate_languages_ratios(text):  
                 languages_ratios = {}
                 tokens = wordpunct_tokenize(text)
                 words = [word.lower() for word in tokens]    
                 for language in stopwords.fileids():
                     stopwords_set = set(stopwords.words(language))
                    
                     words_set = set(words)
                     common_elements = words_set.intersection(stopwords_set)
             
                     languages_ratios[language] = len(common_elements) 
                 return languages_ratios
             
             
             
             def detect_language(text):
                 ratios = _calculate_languages_ratios(text)
                 most_rated_language = max(ratios, key=ratios.get)
                 stop_words_for_language = set(stopwords.words(most_rated_language))
                 return most_rated_language,stop_words_for_language
                 <hr class ="new3"/>
              (3) Preprocessing of Text
             
             
             
             def Preprocessing_Text(Raw_text, stop_words_for_language):
                 
                  1 making text as a space seperated word list
                 stop_words_for_language = str(stop_words_for_language).lower()
                 Words_in_text =[]
                 for word in Raw_text.split():                    
                     Words_in_text.append(word)
             
                 
                  2 remove numbers and special charactes from words
                     
                 alphawords_only = [word for word in Words_in_text if word.isalpha()]          
                 
                 3 removing length 1 words
                 
                 Words_afterRemoval_onelength = [word for word in alphawords_only if len(word)>1]
             
                 4 lower case all words
                 
                 lower_case_only = [word.lower() for word in Words_afterRemoval_onelength ]
                 
                  Remove stopwords 
                 
                 stopwords_nltk = set(stopwords.words("English"))  
                 words_withoutStopwords = [word for word in lower_case_only if word not in stopwords_nltk]
                 if stop_words_for_language != "english":
                     words_withoutStopwords = [word for word in words_withoutStopwords if word not in stop_words_for_language]
                 
                 removing words from common nouns like thank, use, gift, close
                 
                 words_withoutCommonNouns = [word for word in words_withoutStopwords if word not in Common_Nouns ]
                 
                 return list of preprocess words
                 
                 return (words_withoutCommonNouns)
                 <hr class ="new3"/>
              (4) Calculate Frequency of candidate words in text
            
             def Calc_words_frequency(Text_words):
                 
                 Sorted_WordCount_dict ={}  
                 word_and_fr_list=[]
                 Count_fr = Counter(Text_words)    
                 
                 for word,word_count in Count_fr.most_common():
                     word_and_fr_list.append([word, word_count])
                     Sorted_WordCount_dict[word]= word_count
                     
                 return(Sorted_WordCount_dict)
             
                 <hr class ="new3"/>
                 (5) FEATURES Formation
                 #10 features: URL: host, query parts, Headers(h1--H6), Title tag, Anchor tag
             
             def Function_ParseURL(URL):
                 URL =str(URL)
                 host=[]
                 obj=urlparse(URL)    
                 name =(obj.hostname)
                 if len(name)>0:
                     for x in name.split('.'):
                         if x.lower() not in URL_CommonQueryWords:
                             host.append(x)
                     else:
                         host.append(name)
                 path=[]
                 host_part_URL =[]
                       
                 for url_parts in URL.split('/'):
                     for url_part in url_parts.split('.'):            
                         if (len(url_part)>0):
                             for url_words in url_part.split('-'):
                                 if url_words.lower() not in URL_CommnWords and url_words.lower() not in host: 
                                     path.append(url_words.lower())
                         else:
                             path.append(url_parts)                
                 return(host,path)
                 <hr class ="new3"/>

             def function_TexDic_Filter(Tag_TextDic):
                 alt_words=[]
                 if len(Tag_TextDic) > 0:
                     for k,i in Tag_TextDic.items():    
                
                         for x in i:
                             word=[n for n in x.split(',')]
                             for x in word:
                                 words=[i for i in x.split() ]
                                 for x in words:
                                     alt_words.append(x)
                     return(alt_words)
                 else:
                     return(alt_words)
                 
             def function_Tag_Text(Raw_HTML_Soup,Tag_name):
                 TagTextList=[]  
                 for text in Raw_HTML_Soup.find_all(Tag_name):
                     tag_text = text.text.strip().lower()
                     TagTextList.append(tag_text)
                 return TagTextList   
             
             def function_HeaderTitleAnchorText(Raw_HTML_Soup):    
                 H1_TextList = function_Tag_Text(Raw_HTML_Soup,'h1')
                 H2_TextList = function_Tag_Text(Raw_HTML_Soup,'h2')
                 H3_TextList= function_Tag_Text(Raw_HTML_Soup,'h3')
                 H4_TextList = function_Tag_Text(Raw_HTML_Soup,'h4')
                 H5_TextList = function_Tag_Text(Raw_HTML_Soup,'h5')
                 H6_TextList = function_Tag_Text(Raw_HTML_Soup,'h6')
                 Title_TextList = function_Tag_Text(Raw_HTML_Soup,'title')
                 Anchor_TextList = function_Tag_Text(Raw_HTML_Soup,'a')
                 return (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,H6_TextList,
                 Title_TextList,Anchor_TextList)
                 
                 
             def function_MakeDictTagText(Raw_HTML_Soup):
                  
                (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,
                H6_TextList,Title_TextList,Anchor_TextList) = function_HeaderTitleAnchorText(Raw_HTML_Soup)
                     
                 H1_TextDict = {}
                 H2_TextDict = {}
                 H1_TextDict = {}
                 H3_TextDict = {}
                 H4_TextDict = {}
                 H5_TextDict = {}
                 H6_TextDict= {}
                 Title_TextDict = {}
                 Anchor_TextDict = {}
                     
                 H1_TextDict["h1"] = H1_TextList
                 H2_TextDict["h2"] = H2_TextList
                 H3_TextDict["h3"] = H3_TextList
                 H4_TextDict["h4"] = H4_TextList
                 H5_TextDict["h5"] = H5_TextList
                 H6_TextDict["h6"] = H6_TextList    
                 Title_TextDict["title"] = Title_TextList
                 Anchor_TextDict["a"] = Anchor_TextList
                 
                 H1_dic = function_TexDic_Filter(H1_TextDict)
                 H2_dic = function_TexDic_Filter(H2_TextDict)
                 H3_dic = function_TexDic_Filter(H3_TextDict)
                 H4_dic = function_TexDic_Filter(H4_TextDict)
                 H5_dic = function_TexDic_Filter(H5_TextDict)
                 H6_dic = function_TexDic_Filter(H6_TextDict)
                 Title_dic = function_TexDic_Filter(Title_TextDict)
                 Anchor_dic = function_TexDic_Filter(Anchor_TextDict)
                 
                 return (H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic)
                 <hr class ="new3"/>

              (6) Score each candidate words based on Appearance in the features list
                    Manual scoring H1,title, url-Host: 5, h2,url-Query: 4, anchor, h4,h5,h6:2
            
             def Feature_Score(candidate_word,feature_words,score):
                 total_score=0
                 score_single_time =0    
                 for word_feature in feature_words:        
                     if word_feature ==candidate_word:            
                         total_score+=score
                         score_single_time = score                
                 return(score_single_time)
                        
             def Tf_Score(fr,text_length):
                 if text_length < 50:
                     tf_score =((fr/100)*50)
                 else:
                     tf_score=((fr/100)*20) 
             
             def function_word_Fr_TagName_ScoreDic(words_count_dic, text_length):
                 wrd_fr_Tgs_Fnl_score =defaultdict()
                 Word_Final_Score =defaultdict()
                 
                 names of features 10
                 Name_FeaturesList =np.array(['H1', 'H2', 'H3','H4', 'H5', 'H6','Title','Anchor','URL-H','URL-Q'])
                 
                  Manual score for words
                 Manual_Score_Each_Features =np.array([6, 5, 4,3, 2, 2, 6, 1,5,4])
                 
                 
                 
                  #Get all the words in features
                 
                 (H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic)= function_MakeDictTagText(Raw_HTML_Soup)
                 featuresText_allDict_npArrayList = np.array([H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic, Host_part_of_URL, Query_part_of_URL])
                
                 
                 for word,fr in words_count_dic.items():
                     tf_score = Tf_Score(fr,text_length)
                     tag =[]
                     name_tag =[]
                            
                     for word_inAll_Dic in range (len(featuresText_allDict_npArrayList)):
                         if word in featuresText_allDict_npArrayList[word_inAll_Dic]:   
                             tag.append(Manual_Score_Each_Features[word_inAll_Dic]) 
                             name_tag.append(Name_FeaturesList[word_inAll_Dic])
                     score= (sum(tag))
                     score = score + tf_score
                     Word_Final_Score[word] = score
                     wrd_fr_Tgs_Fnl_score[word] = fr,name_tag,score
                 return (wrd_fr_Tgs_Fnl_score, Word_Final_Score)

               <hr class ="new3"/>
              (7) Ranking and Selection of Final 1o keywords main calling
            
             if __name__ == "__main__":
                 
                 URL ="http://bbc.com"
                 
                 (Raw_text, Raw_HTML_Soup) = Web_Funtion(URL)
                 most_rated_language,stop_words_for_language = detect_language(Raw_text)
                 
                 preprocess_TextWords = Preprocessing_Text(Raw_text, stop_words_for_language )
                 text_length = len(preprocess_TextWords)
                 words_count_dic = Calc_words_frequency(preprocess_TextWords)
                 
                 
                  Features
                 Host_part_of_URL, Query_part_of_URL = Function_ParseURL(URL)
                 
                 (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,H6_TextList,Title_TextList,Anchor_TextList) = function_HeaderTitleAnchorText(Raw_HTML_Soup)
                
                 Feature Header, Title, Anchor text, score dictionary
                 (wrd_fr_Tgs_Fnl_score, Word_Final_Score) = function_word_Fr_TagName_ScoreDic(words_count_dic, text_length)   
                 
                
             
                 keyword =[]
                 sorted_word_score = Counter(Word_Final_Score)
                 for word,score in sorted_word_score.most_common(10):
                     keyword.append(word)
                 print (keyword)
                 
              <hr class ="new3"/>
                                      Ends here
              <hr class ="new3"/>
                 </code></pre>
              
                
                    </code></pre>
          </div>
          <div class="output">
            <hr/>  <h4>(4) Outpu Section</h4><hr>
            <div class="result">
            
             
                          <h4>(1) Extract Text,Raw HTML Webpage Output</h4>
              <hr class ="new3"/>
                  
                  <div class="output_example">
                    <h4>Raw Text</h4><pre><code>
                      html
                      BBC - Homepage
                      window.orb_fig_blocking = true;
                      window.bbcredirection = {geo: true};
                      :root {
                      --bbc-font: ReithSans, Arial, Helvetica, freesans, sans-serif;
                      --bbc-font-legacy: Arial, Helvetica, freesans, sans-serif;
                      }
                      window.orbitData = {};
                      var additionalPageProperties = {};
                      additionalPageProperties['custom_var_1'] = 'international' || null;
                      additionalPageProperties['custom_var_9'] = '1' || null;
                      additionalPageProperties['experience_global_platform'] = 'orbit';
                      window.orbitData.userProfileUrl = "https://www.bbc.co.uk/userprofile";
                      window.page = {
                      name: 'home.page' || null,
                      destination: 'HOMEPAGE_GNL' || null,</code></pre>
                      
                      <h4>Text of Webpage</h4>
                      <br>
                      <pre><code>Homepage Accessibility links Skip to content Accessibility Help BBC Account require(['idcta/statusbar'], function (statusbar) {new statusbar.Statusbar({id: 'idcta-statusbar', publiclyCacheable: true});}); Notifications Home News Sport Weather iPlayer Sounds Bitesize CBeebies CBBC Food Home News Sport Reel Worklife Travel Future Culture TV Weather Sounds More menu
                        Search BBC
                        Search BBC
                        Home News Sport Weather iPlayer Sounds Bitesize CBeebies CBBC Food Home News Sport Reel Worklife Travel Future Culture TV Weather Sounds Close menu
                        BBC Homepage
                       
                        Charles and Camilla crowned in historic Coronation
                        King Charles and Queen Camilla have been crowned on a day of pageantry, history - and downpours.
                        UK
                        Charles and Camilla crowned in historic Coronation
                        The story of Coronation day in extraordinary photos
                        News
                        The story of Coronation day in extraordinary photos
                        Prince Harry leaves alone after Coronation
                        UK
                        Prince Harry leaves alone after Coronation
                        Dozens of protesters arrested during Coronation
                        UK
                        Dozens of protesters arrested during Coronation
                        Real Madrid win first Copa del Rey since 2014
                        European Football
                        Real Madrid win first Copa del Rey since 2014</code></pre>
               
                  </div>
                  <hr class ="new3"/>
                  <h4> (2) Language Detection</h4>
              
                  <hr class ="new3"/>
                  <div class="output_container">English Language</div>
                  <hr class ="new3"/>
                  <h4>(3) Preprocessing Text Output</h4>
                  <hr class ="new3">
              <pre><code>BBC -Homepage Homepage Accessibility links Skip to content
                Accessibility Help BBC Account Notifications Home News Sport Weather
                iPlayer Sounds Bitesize CBeebies CBBC Food Home News Sport Reel
                Worklife Travel Future Culture TV Weather Sounds More menu Search
                BBC Search BBC Home News Sport Weather iPlayer Sounds Bitesize
                CBeebies CBBC Food Home News Sport Reel Worklife Travel Future
                Culture TV Weather Sounds Close menu BBC Homepage Putin should be</code></pre>
                  <hr class ="new3"/>
                  <div class="output_container">
                    <div class="output_example">
                      <pre><code>
                      Total words in a webpage : 9496 
                      Length of Words length after removing 1 length words: 1537        
                      Words length After numbers, structure removal:1349
                      Words after removing special characters:1319
                      After removing stopwords: 945
                      After removing common nouns length of Final text:922
                    </code></pre>
              
                    
              
                      
                    </div>
                  </div>
                  <hr class ="new3">
              <h4> (4) Candidate Keywords</h4>
              <hr class ="new3">
                  <div class="output_example">
                    BBC -Homepage Homepage Accessibility links Skip to content
                    Accessibility Help BBC Account Notifications Home News Sport Weather
                    iPlayer Sounds Bitesize CBeebies CBBC Food Home News Sport Reel
                    Worklife Travel Future Culture TV Weather Sounds More menu Search
                    BBC Search BBC Home News Sport Weather iPlayer Sounds Bitesize
                    CBeebies CBBC Food Home News Sport Reel Worklife Travel Future
                    Culture TV Weather Sounds Close menu BBC Homepage Putin should be
                    sentenced for 'criminal actions' - Zelensky The Ukrainian president
                    calls for the creation of a new war tribunal as he addresses The
                    Hague. Europe Putin should be sentenced for 'criminal actions' -
                    Zelensky 
                  </div>
                  <hr class ="new3"/>
                  <h4>(5) Feature Formation</h4>
              
                  <hr class ="new3"/>
                  <div class="output_container">
                    <br />Headers <br />H1<br />
              
                    ['bbc homepage']<br />
              
                    H2<br />
              
                    ['accessibility links', '', 'news', 'sport', 'coronation of king
                    charles iii', 'london weather', 'editorâs picks', 'latest business
                    news', 'technology of business', 'advertisement', 'new tech
                    economy', 'featured video', 'bbc world service', 'more around the
                    bbc', 'from our correspondents', 'global trade', 'new tech economy',
                    'world in pictures', 'bbc in other languages', 'more languages',
                    'explore the bbc'] 
                    
                    H3<br />
              
                    ['us denies masterminding moscow drone attack', 'top us judge under
                    fresh scrutiny over school fees', 'ed sheeran wins thinking out loud
                    copyright case', 'what side-hustlers are really making', 'the true
                    story of the kentucky derby', 'four proud boys guilty of seditious
                    conspiracy', 'silence and teddies at scene of serbia school
                    shooting', 'prince william and kate drop into a soho pub', 'serie a:
                    napoli bidding to clinch title but go 1-0 down at udinese', 'a tour
                    of a lost world, before football changed forever', 'premier league:
                    brighton 0-0 man utd - rashford goes close to opener', 'what kings
                    wore from tudor times to now', "the 'super-deep' diamonds in the
                    crown", 'your full guide to how coronation day will unfold', 'thu',
                    'fri', 'sat', 'sun', 'a misunderstood ancient erotic manual', 'why
                    do french men pee on the street?', 'the surprisingly deadly secret
                    of the grapefruit', 'a regal scone made for king charles iii', 'do
                    you own too many clothes?', 'can remote-work gossip backfire?', 'the
                    rappers risking the death penalty', 'why the wicker man has divided
                    opinion for 50 years', 'camilla: from tabloid target to crowned
                    queen', "lizzo thanks 'king of flutes' for met gala duet", 'shell
                    reports stronger than expected profits', 'us raises interest rates
                    to highest in 16 years', 'investors sue over credit suisse
                    collapse', 'china tourism rebounds above pre-pandemic levels',
                    "branson feared 'losing everything' in pandemic", 'the revival of a
                    historic italian fruit', 'the first climate-resilient nation?', 'a
                    major positive climate tipping point', 'why there is serious money
                    in kitchen fumes', 'the people turning time into a currency',
                    "ukraine's first lady and pm's wife embrace outside no 10",
                    "ukraine's first lady and pm's wife embrace...", 'how well does
                    william pull a pint?', 'space trash floats away during spacewalk',
                    "ros atkins on... the videos showing 'kremlin...", "russian media's
                    muted response to kremlin...", 'inside hospital where oxygen runs
                    out', 'which route will the king take to his', 'watch man
                    
                    'spanish']<br />
                    H4<br />
              
                    [EMPTY] <br />H5<br />
              
                    [EMPTY] <br />H6<br />
              
                    [EMPTY]
                  </div>
              
                  <hr class ="new3"/>
                  <h4>(6) Feature Score Output</h4>
              
                  <hr class ="new3"/>
                  <div class="output_container">
                    <pre>
                    +---------------+-----------+------------------------------------------+--------------------+
              |      Word     | Frequency |                   TAGS                   |    Final-Score     |
              +---------------+-----------+------------------------------------------+--------------------+
              |      bbc      |     14    | ['H1', 'H2', 'Title', 'Anchor', 'URL-H'] |        25.8        |
              |    function   |     10    |                    []                    |        2.0         |
              |    weather    |     9     |          ['H2', 'H3', 'Anchor']          |        11.8        |
              |      news     |     8     |             ['H2', 'Anchor']             |        7.6         |
              |     sport     |     8     |             ['H2', 'Anchor']             |        7.6         |
              |    business   |     8     |             ['H2', 'Anchor']             |        7.6         |
              |     return    |     7     |                    []                    | 1.4000000000000001 |
              |     watch     |     7     |             ['H3', 'Anchor']             |        6.4         |
              |      home     |     6     |                ['Anchor']                |        2.2         |
              |     sounds    |     6     |                ['Anchor']                |        2.2         |
              |     travel    |     6     |                ['Anchor']                |        2.2         |
              |     future    |     6     |                ['Anchor']                |        2.2         |
              |    charles    |     6     |          ['H2', 'H3', 'Anchor']          |        11.2        |
              |     typeof    |     5     |                    []                    |        1.0         |
              |    worklife   |     5     |                ['Anchor']                |        2.0         |
              |    culture    |     5     |                ['Anchor']                |        2.0         |
              |    football   |     5     |             ['H3', 'Anchor']             |        6.0         |
              |      tech     |     5     |          ['H2', 'H3', 'Anchor']          |        11.0        |
              |    homepage   |     4     |        ['H1', 'Title', 'Anchor']         |        13.8        |
              |      reel     |     4     |                ['Anchor']                |        1.8         |
              |     denies    |     4     |             ['H3', 'Anchor']             |        5.8         |
              |     europe    |     4     |                ['Anchor']                |        1.8         |
              |      top      |     4     |             ['H3', 'Anchor']             |        5.8         |
              |     school    |     4     |             ['H3', 'Anchor']             |        5.8         |
              | entertainment |     4     |                ['Anchor']                |        1.8         |
              |      arts     |     4     |                ['Anchor']                |        1.8         |
              |     years     |     4     |             ['H3', 'Anchor']             |        5.8         |
              |    william    |     4     |             ['H3', 'Anchor']             |        5.8         |
              |     makes     |     4     |             ['H3', 'Anchor']             |        5.8         |
              |      lady     |     4     |             ['H3', 'Anchor']             |        5.8         |
              |      wife     |     4     |             ['H3', 'Anchor']             |        5.8         |
              |    russian    |     4     |             ['H3', 'Anchor']             |        5.8         |
              |     photos    |     4     |             ['H3', 'Anchor']             |        5.8         |
              |   primitive   |     3     |                    []                    |        0.6         |
              |     catch     |     3     |                    []                    |        0.6         |
              |     event     |     3     |                    []                    |        0.6         |
              |      set      |     3     |                    []                    |        0.6         |
              | optimizelyurl |     3     |                    []                    |        0.6         |
              | accessibility |     3     |             ['H2', 'Anchor']             |        6.6         |
              |    iplayer    |     3     |                ['Anchor']                |        1.6         |
              |    bitesize   |     3     |                ['Anchor']                |        1.6         |
              |    cbeebies   |     3     |                ['Anchor']                |        1.6         |
              |      cbbc     |     3     |                ['Anchor']                |        1.6         |
              |      food     |     3     |                ['Anchor']                |        1.6         |
              |       tv      |     3     |                ['Anchor']                |        1.6         |
              |     attack    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    knowing    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     guilty    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |   seditious   |     3     |             ['H3', 'Anchor']             |        5.6         |
              |   conspiracy  |     3     |             ['H3', 'Anchor']             |        5.6         |
              |   copyright   |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     serie     |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     napoli    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    udinese    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |      tour     |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    premier    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    brighton   |     3     |             ['H3', 'Anchor']             |        5.6         |
              |      iii      |     3     |          ['H2', 'H3', 'Anchor']          |        10.6        |
              |      soho     |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     royal     |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    diamonds   |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     videos    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |     change    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |   technology  |     3     |             ['H2', 'Anchor']             |        6.6         |
              |    branson    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    embrace    |     3     |             ['H3', 'Anchor']             |        5.6         |
              |    science    |     3     |                ['Anchor']                |        1.6         |
              |   hollywood   |     3     |             ['H3', 'Anchor']             |        5.6         |
              </pre>
                  </div>
                  <hr class ="new3"/>
                  <h4>(7) Final Keywords Output</h4>
              
                  <hr class ="new3"/>
                  <pre> <code>
                    bbc
                    coronation
                    homepage
                    charles
                    iii
                    global
                    tech
                    news
                    sport
                    business
                  </code> </pre>
                  <hr class ="new3">
                  <h4>Result Drank Ends</h4>
                  
                  <hr class ="new3">
              <hr>
            </div>
          </div>
        </div>
      </section>

      <section id="Webrank">
        <div class="container">
          <div class="title">
            <h2>WebRank</h2>
            <article>
              <article>
                <p>
                  In this WebRank section we  we will discuss four main parts:<br/>
                   (1) Introduction (2) Methodology:
                   (3) Python Implementation (4) Output Section 
               
                </p>
                <div class="title"><h3> (1) Introduction </h3></div>
              <p>

</p>               
                

                
              
            </article>
          </div>
          <div class="methodology">
            <div class="title"><h3> (2) Methodology </h3></div>
            <article>
              <p>Fig. 1. presents the workflow of the proposed keywords extraction method. The method has two modules: (1) pre-processing and (2) keyword extraction. The pre-processing module involves the extraction of the natural language text from the web page. The keyword extraction module utilizes the text from the pre-processing module. 
                In the pre-processing module, the first three functions involve the filtering of the text from all the other content of a web page. All the content of a web page is extracted using a document object model (DOM) and X-path function. The text that belongs to javascript scripting language and cascade style sheets is eliminated in the text filtering function. The special characters, such as @,*,Â£, or $, punctuation marks, and numbers are also filtered out using the regular expression in the text filtering function. Similarly, the text filtering function also involves the removal of the stop words from the text. The stop words are the natural language words that have minimal or no meaning, such as and, the, a, and an. The filtered text can now be utilized for natural language processing.
                The POS extractor, normalize text, and separate POS functions involve the natural language processing on the filtered text. The POS extractor function divides the text into tokens. A token is a whitespace-separated unit of text. The tokens are assigned the POS tags, such as nouns, adjectives, and verbs. 
                <p>  <img
                  src="./images/WebRank_workflow.PNG"
                  alt="Dranks"
                  width="600"
                  height="300"
                />
        <p></p>
            </article>
            <div class="title"><h3> (3) Python Implementation</h3></div>
            <p><ol>
             <li>Extract Text</li>
             <li>Preprocess Text</li>
             <li>POS Tags Seperation</li>
             <li>Word Net semantic similarity</li>
             <li>Cluster Words</li>
             <li>Keyword Ranking and Selection</li>
           </ol></p>
          </div>
          <div class="coding" >
   
            <pre><code>
              
              <h4> Import packages</h4>
              
              <code> Imports
                import urllib
                import nltk
                import sys
                import re                 
                import lxml
                import math
                import string
                import textwrap
                import requests                
                from nltk.corpus import stopwords
                from bs4 import BeautifulSoup
                from nltk import word_tokenize
                from nltk.stem import WordNetLemmatizer
                from collections import defaultdict,Counter
                from nltk.corpus import stopwords
                from collections import defaultdict 
                from bs4.element import Comment                
                from nltk import wordpunct_tokenize
                from urllib.parse import urlparse                 
                import pandas as pd 
                import numpy as np                
                Common_Nouns ="january debt est dec big than who use jun jan ".split(" ")
                URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
                URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
                UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
                
                <hr class ="new3"/>
        <h4>(1) Extract Text of Webpage</h4>
       
                
                def Scrapper1(element):
                    if element.parent.name in [UselessTagsText]:
                        return False
                    if isinstance(element, Comment):
                        return False
                    return True
                
                def Scrapper2(body):             
                    soup = BeautifulSoup(body, 'lxml')      
                    texts = soup.findAll(text=True)   
                    name =soup.findAll(name=True) 
                    visible_texts = filter(Scrapper1,texts)        
                    return u" ".join(t.strip() for t in visible_texts)
                
                def Scrapper3(text):                  
                    lines = (line.strip() for line in text.splitlines())    
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    return u'\n'.join(chunk for chunk in chunks if chunk)
                
                
                def Scrapper_title_4(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()
                  title=[]
                  
                  soup = BeautifulSoup(html, 'lxml') 
                  title.append(soup.title.string)
                  return(title,urls)
                
                def Web_Funtion(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()  
                  Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
                 
                  raw =Scrapper2(html)
                  Raw_text = Scrapper3(raw) 
                  return(Raw_text,Raw_HTML_Soup)  
                
              
                
                    </code></pre>
          </div>
          <div class="output">
            <div class="result">
              <h4></h4>
              <pre><code></code></pre>
            </div>
          </div>
        </div>
      </section>

      <section id="Acirank">
        <div class="container">
          <div class="title">
            <h2>ACI-Rank</h2>
            <article>
              
                <p>
                  In this Hrank section we  we will discuss four main parts:<br/>
                   (1) Introduction (2) Methodology:
                   (3) Python Implementation (4) Output Section 
               
                </p>
                <div class="title"><h3> (1) Introduction </h3></div>
              <p>
                we study the importance of the distribution of semantically similar POS tags, such as nouns, adjectives, and verbs in the extraction of relevant keywords from the web page
              </p><p><li>	A new keywords extraction method that requires a minimum knowledge of DOM structure. </li>
                <li>A simple measure TF performs better than the more complex methods. </li>
                  <li>However, the combination of nouns, adjectives, and verbs improves performance when TF fails.</li>
                 
<li> A new keywords extraction method that requires a minimum knowledge of DOM structure. </li>
<li>The proposed method outperforms CL-Rank, TextRank, and TF. A simple measure TF performs better than the more complex methods. However, the combination of nouns, adjectives, and verbs improves performance when TF fails.</li>
</p>               
                

                
              
            </article>
          </div>
          <div class="methodology">
            <div class="title"><h3> (2) Methodology </h3></div>
            <article>
              <p>Fig. 1. presents the workflow of the proposed keywords extraction method. The method has two modules: (1) pre-processing and (2) keyword extraction. The pre-processing module involves the extraction of the natural language text from the web page. The keyword extraction module utilizes the text from the pre-processing module. 
                In the pre-processing module, the first three functions involve the filtering of the text from all the other content of a web page. All the content of a web page is extracted using a document object model (DOM) and X-path function. The text that belongs to javascript scripting language and cascade style sheets is eliminated in the text filtering function. The special characters, such as @,*,Â£, or $, punctuation marks, and numbers are also filtered out using the regular expression in the text filtering function. Similarly, the text filtering function also involves the removal of the stop words from the text. The stop words are the natural language words that have minimal or no meaning, such as and, the, a, and an. The filtered text can now be utilized for natural language processing.
                The POS extractor, normalize text, and separate POS functions involve the natural language processing on the filtered text. The POS extractor function divides the text into tokens. A token is a whitespace-separated unit of text. The tokens are assigned the POS tags, such as nouns, adjectives, and verbs. 
                <p>  <img
                  src="./images/Hrank_workflow.PNG"
                  alt="Dranks"
                  width="600"
                  height="300"
                />
                <p>Fig. 1. Workflow of Drank</p></p>
                <p>The tokens with POS tags are further normalized.
                 The normalization is the process of replacing the inflected forms of a word with the root word. The inflected form represents the different usage of a word in the sentences. For example, finds, finding, and found are the inflected forms of the word find. An inflected form of a word has a changed spelling or ending. In natural language processing, the lemmatization is used to find the inflected form of the words with different spellings, such as finds and found for the word find in the above example. Unlike lemmatization, the stemming process takes care of the prefixes and suffixes to find the root word, such as finding in the abovementioned example. The output of the normalization process is the tokens with all the inflected forms replaced with their root word.
                The lists of the POS-tagged tokens are provided to the separate POS function, which separates the tokens into the lists of nouns, adjectives, and verbs. The lists are provided to the count frequency function. The count frequency function calculates the frequency of the words in the separate lists having nouns, adjectives, and verbs. The top-frequent tokens are selected as candidate keywords. The semantically similar words among top-frequent tokens are grouped together using a lexical database, named as WordNet. The lexical database helps in finding the synsets of the words. The synset is a set of one or more synonyms that can be used interchangeably in some context [20].                                                                                                                                                                                                                                                                                                                                                                                                                            
              </p>
              <p>
              We compute the semantic similarity of two different words using path-similarity, which is based on the WordNet [21]. The words that have no synonyms in the WordNet are removed from the lists. The path-similarity metric calculates the score between two different words in terms of their relatedness. We use path-similarity because it is very simple and it operates based on a parent-child relationship like a tree. Therefore, it is more convenient to use in our case.
                Three similarity matrices are created independently for the nouns, adjectives, and verbs. The similarity matrices are utilized in clustering the related words. We use an agglomerative clustering to find similar words in the lists.  The clusters are scored by counting the frequencies of all the words in each cluster. The clusters are ranked according to the scores.
                </p>  
            </article>
            <div class="title"><h3> (3) Python Implementation</h3></div>
            <p><ol>
             <li>Extract Text</li>
             <li>Preprocess Text</li>
             <li>POS Tags Seperation</li>
             <li>Word Net semantic similarity</li>
             <li>Cluster Words</li>
             <li>Keyword Ranking and Selection</li>
           </ol></p>
          </div>
          <div class="coding" >
   
            <pre><code>
              
              <h4> Import packages</h4>
              
              <code> Imports
                import urllib
                import nltk
                import sys
                import re 
                
                import lxml
                import math
                import string
                import textwrap
                import requests
                
                from nltk.corpus import stopwords
                from bs4 import BeautifulSoup
                from nltk import word_tokenize
                from nltk.stem import WordNetLemmatizer
                from collections import defaultdict,Counter
                from nltk.corpus import stopwords
                from collections import defaultdict 
                from bs4.element import Comment
                
                from nltk import wordpunct_tokenize
                from urllib.parse import urlparse 
                
                import pandas as pd 
                import numpy as np
                
                Common_Nouns ="january debt est dec big than who use jun jan feb mar apr may jul agust dec oct nov sep dec  product continue second secodns".split(" ")
                URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
                URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
                UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
                <hr class ="new3"/>
        <h4>(1) Extract Text of Webpage</h4>
       
                
                def Scrapper1(element):
                    if element.parent.name in [UselessTagsText]:
                        return False
                    if isinstance(element, Comment):
                        return False
                    return True
                
                def Scrapper2(body):             
                    soup = BeautifulSoup(body, 'lxml')      
                    texts = soup.findAll(text=True)   
                    name =soup.findAll(name=True) 
                    visible_texts = filter(Scrapper1,texts)        
                    return u" ".join(t.strip() for t in visible_texts)
                
                def Scrapper3(text):                  
                    lines = (line.strip() for line in text.splitlines())    
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    return u'\n'.join(chunk for chunk in chunks if chunk)
                
                
                def Scrapper_title_4(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()
                  title=[]
                  
                  soup = BeautifulSoup(html, 'lxml') 
                  title.append(soup.title.string)
                  return(title,urls)
                
                def Web_Funtion(URL):
                  req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
                  con = urllib.request.urlopen(req)
                  html= con.read()  
                  Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
                 
                  raw =Scrapper2(html)
                  Raw_text = Scrapper3(raw) 
                  return(Raw_text,Raw_HTML_Soup)  
                
              
                
                    </code></pre>
          </div>
          <div class="output">
            <div class="result">
              <h4></h4>
              <pre><code></code></pre>
            </div>
          </div>
        </div>
      </section>
    </main>
    <script src ="app.js"></script>
  </body>
</html>
