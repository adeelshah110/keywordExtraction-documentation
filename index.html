<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project 8</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <header>
      <nav>
        <h1>Keyword Extraction</h1>
        <ul>
          <a href="#Introduction"><li>Introduction</li></a>

          <a href="#Hrank"><li>Hrank</li></a>

          <a href="#Drank"><li>Drank</li></a>

          <a href="#Webrank"><li>Webrank</li></a>

          <a href="#Acirank"><li>ACI-rank</li></a>
        </ul>
      </nav>
    </header>
    <main>

      <section id="Introduction">
        <div class="title">
        <h2>About Keyword Extraction</h2>
      </div>
        <article>
          <p>
            Google defnes a keyword as an isolated word or phrase that provides
            concise high-level information about content to readers . With the
            increasing amount of data, users need more resources and time to
            understand content. Keywords make it easier to understand the
            meaning of a text in fewer words.
          </p>
          <p>
            In short, keywords summarize the key points presented in the text.
            When searching for information on search engines, keywords play a
            signifcant role in fnding relevant content. Keywords are the
            mostinformative part of a text; they are the most prominent words in
            the text and describe its content. Keywords are necessary in
            situations involving huge amounts of text data that need to be
            processed automatically. Keywords are widely used in document
            summarization, indexing, categorization, and clustering of huge
            datasets. Many scientifc publications contain keyword lists that
            have been explicitly assigned by their authors. Other documents,
            however, have not been assigned keywords . As webpages are
            constantly updated, it is difcult to create keywords manually.
            Manual keyword assignment is labor intensive, time consuming, and
            error prone.
          </p>

          <p>
            Specialized curators use fxed taxonomies for manual keyword
            generation, but in some cases, the keywords chosen by the author are
            not sufciently comprehensive and accurate. Without high-quality
            keywords, users fail to catch relevant information . Keywords ofer
            readers a concise high-level summary of a documents content, thereby
            improving their understanding of that text. Keywords are the most
            relevant and important indicator for users seeking to grasp the
            fundamentals of a topic when scanning or skimming an article.
            Keyword extraction is a basic step in many text-mining and natural
            language processing (NLP) techniques, including text summarization,
            information retrieval, topic modeling, clustering, and content-based
            advertisement systems. Finding the relevant webpages, a user is
            seeking is often a challenging task for which representative
            keywords or keyphrases.
          </p>

          <p>
            This article addresses the issue of automatic extracting keywords
            from webpages. The majority of existing keyword extraction methods
            use language-dependent Natural Language Processing (NLP) based
            techniques, including Part-of-Speech (POS) tagging, stemming, and
            lemmatization, which makes it complex to generalize a method for
            different languages. The main purpose of the research is to extract
            only those language-independent features of web pages in order to
            find a method that can be applied in different languages.
          </p>
          <p>
            So far, studies on language-independent approaches have been limited
            because they usually perform worse than methods that take advantage
            of linguistic features. Extracting keywords from web documents
            involves two main challenges: the first is the presence of noise and
            irrelevant data such as navigation bars, menus, comments and
            advertisements and the second consists in the presence of multiple
            topics and multiple languages. Therefore, it is very important to
            have a general keyword extraction method that can extract keywords
            without relying on any specific language.
          </p>

          <p>
            addresses the challenges of keyword extraction by developing and
            testing four new techniques, both language-dependent and
            language-independent as well as supervised and unsupervised. Special
            attention is paid to finding the most relevant features for
            identifying good keyword and keyphrase candidates. The work deals
            with statistical, linguistic, and structural features as well as
            their combinations. This diversity of approaches serves the
            pragmatic overall goal of finding the best available methods by
            assessing the relative performances of the newly developed as well
            as existing methods on a number of different datasets.
          </p>

          <p>
            For this purpose the author proposes four new automatic keyword
            extraction methods for webpages: Hrank, D-rank, WebRank, and
            ACI-rank
          </p>
        </article>
      </section>
<section id="Hrank">
  <div class="container">
    <div class="content">

      <div class="title">
        <h2>HRANK </h2>       
      </div>
      <div class = "intro">
      <h4  class = "introduction">Introduction</h4> 
      <article><p></p></article>
        </div>
       
        <div class="methodology">

        <h4 class = "introduction">Methodology</h4> 
        <article><p></p></article>
      </div>
     
      
        <div class="implementation">
        
        <p><ol>
          <li>Extract Text</li>
          <li>Preprocess Text</li>
          <li>POS Tags Seperation</li>
          <li>Word Net semantic similarity</li>
          <li>Cluster Words</li>
          <li>Keyword Ranking and Selection</li>
        </ol></p>
        <pre><code>
          
          <h4> Import packages</h4>
          
          <code># Imports
            import urllib
            import nltk
            import sys
            import re 
            
            import lxml
            import math
            import string
            import textwrap
            import requests
            
            from nltk.corpus import stopwords
            from bs4 import BeautifulSoup
            from nltk import word_tokenize
            from nltk.stem import WordNetLemmatizer
            from collections import defaultdict,Counter
            from nltk.corpus import stopwords
            from collections import defaultdict 
            from bs4.element import Comment
            
            from nltk import wordpunct_tokenize
            from urllib.parse import urlparse 
            
            import pandas as pd 
            import numpy as np
            
            Common_Nouns ="january debt est dec big than who use jun jan feb mar apr may jul agust dec oct nov sep dec  product continue second secodns".split(" ")
            URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
            URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
            UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
            <hr/>
    <h4>(1) Extract Text of Webpage</h4>
   
            
            def Scrapper1(element):
                if element.parent.name in [UselessTagsText]:
                    return False
                if isinstance(element, Comment):
                    return False
                return True
            
            def Scrapper2(body):             
                soup = BeautifulSoup(body, 'lxml')      
                texts = soup.findAll(text=True)   
                name =soup.findAll(name=True) 
                visible_texts = filter(Scrapper1,texts)        
                return u" ".join(t.strip() for t in visible_texts)
            
            def Scrapper3(text):                  
                lines = (line.strip() for line in text.splitlines())    
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                return u'\n'.join(chunk for chunk in chunks if chunk)
            
            
            def Scrapper_title_4(URL):
              req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
              con = urllib.request.urlopen(req)
              html= con.read()
              title=[]
              
              soup = BeautifulSoup(html, 'lxml') 
              title.append(soup.title.string)
              return(title,urls)
            
            def Web_Funtion(URL):
              req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
              con = urllib.request.urlopen(req)
              html= con.read()  
              Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
             
              raw =Scrapper2(html)
              Raw_text = Scrapper3(raw) 
              return(Raw_text,Raw_HTML_Soup)  
            
          
            
                </code></pre><hr/>
      </div>
    <hr/>

      <h4 class = "introduction">Output Section Hrank Exmple </h4> 
      
      <div class="output">
        
        <div class="result">
          <h5></h5>
          <pre></pre>
          <h5></h5>
          <pre></pre>
        </div>
      </div>



    </div>
  </div>
</section>

<section id="Drank">
  <div class="container">
    <div class="content">

      <div class="title">
        <h2>DRANK </h2>       
      </div>
      <div class = "intro">
      <h4  class = "introduction">Introduction</h4> 
      <article> <p>
                  Work deals with webpage keyword extraction, which is crucial for the
        information retrieval task performed by search engines browsing
        through the internet. As such, keyword extraction is a specific kind
        of information extraction task, where the use of a natural language,
        or even several languages, poses severe challenges. To conquer these
        challenges, appropriate natural language processing (NLP) techniques
        have to be applied. As the method is dealing with webpages, the task
        is further complicated by the varying structure and layout of the
        pages. Even if Google search is widely and successfully used by a
        vast number of people for all so many purposes, the search results
        are often far from optimal, and processing natural language
        documents remain challenging.
      </p>
      <p>
        The D-rank method is an unsupervised method where the candidate
        keywords were ranked based on their position in the content after
        extracting their features from the DOM structure. The author tested
        the proposed method on a dataset of webpages in three languages:
        English, Finnish, and German.
      </p></article>
        </div>
        
        <div class="methodology">

        <h4 class = "introduction">Methodology</h4> 
       
        <article><p>    <div class="image_container">
          <img
            src="Drank_workflow.PNG"
            alt="Dranks"
            width="600"
            height="300"
          />
          <p>Table 1. Workflow of Drank</p>
        </div></p></article>
      </div>
     
        <div class="implementation">
        <h4 class = "introduction">Python implementaion</h4> 
       <ol>
          <li>Extract Text</li>
          <li>Preprocess Text</li>
          <li>Feature Formation</li>
          <li>Score Feature Words</li>
          <li>Final Keyword Selection</li>
        </ol>
       
    <hr/>
          <h4> Import packages</h4>
          <pre>
          <code># Imports
            import urllib
            import nltk
            import sys
            import re 
            
            import lxml
            import math
            import string
            import textwrap
            import requests
            
            from nltk.corpus import stopwords
            from bs4 import BeautifulSoup
            from nltk import word_tokenize
            from nltk.stem import WordNetLemmatizer
            from collections import defaultdict,Counter
            from nltk.corpus import stopwords
            from collections import defaultdict 
            from bs4.element import Comment
            
            from nltk import wordpunct_tokenize
            from urllib.parse import urlparse 
            
            import pandas as pd 
            import numpy as np
            
            Common_Nouns ="january debt est dec big than who use jun jan feb mar apr may jul agust dec oct nov sep dec  product continue one two three four five please thanks find helpful week job experience women girl apology read show eve  knowledge benefit appointment street way staff salon discount gift cost thing world close party love letters rewards offers special close  page week dollars voucher gifts vouchers welcome therefore march nights need name pleasure show sisters thank menu today always time needs welcome march february april may june jully aguast september october november december day year month minute second secodns".split(" ")
            URL_CommnWords =['','https','www','com','-','php','pk','fi','http:','http']
            URL_CommonQueryWords = ['','https','www','com','-','php','pk','fi','https:','http','http:','html','htm']
            UselessTagsText =['html','style', 'script', 'head',  '[document]','img']
            
          </code>
                </pre>
            <hr/>
    <h4>(1) Extract Text of Webpage</h4>
    <pre><code>
            
            def Scrapper1(element):
                if element.parent.name in [UselessTagsText]:
                    return False
                if isinstance(element, Comment):
                    return False
                return True
            
            def Scrapper2(body):             
                soup = BeautifulSoup(body, 'lxml')      
                texts = soup.findAll(text=True)   
                name =soup.findAll(name=True) 
                visible_texts = filter(Scrapper1,texts)        
                return u" ".join(t.strip() for t in visible_texts)
            
            def Scrapper3(text):                  
                lines = (line.strip() for line in text.splitlines())    
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                return u'\n'.join(chunk for chunk in chunks if chunk)
            
            
            def Scrapper_title_4(URL):
              req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
              con = urllib.request.urlopen(req)
              html= con.read()
              title=[]
              
              soup = BeautifulSoup(html, 'lxml') 
              title.append(soup.title.string)
              return(title,urls)
            
            def Web_Funtion(URL):
              req = urllib.request.Request(URL, headers={'User-Agent' : "Magic Browser"})
              con = urllib.request.urlopen(req)
              html= con.read()  
              Raw_HTML_Soup = BeautifulSoup(html, 'lxml') 
             
              raw =Scrapper2(html)
              Raw_text = Scrapper3(raw) 
              return(Raw_text,Raw_HTML_Soup) 
            
            
            </code></pre>
            
                <hr/>
    <h4> (2) Detect Language of Text</h4>
    <pre><code>
            def _calculate_languages_ratios(text):  
              languages_ratios = {}
              tokens = wordpunct_tokenize(text)
              words = [word.lower() for word in tokens]    
            for language in stopwords.fileids():
                stopwords_set = set(stopwords.words(language))
              
                words_set = set(words)
                common_elements = words_set.intersection(stopwords_set)
    
                languages_ratios[language] = len(common_elements) 
            return languages_ratios
    
            def detect_language(text):
                ratios = _calculate_languages_ratios(text)
                most_rated_language = max(ratios, key=ratios.get)
                stop_words_for_language = set(stopwords.words(most_rated_language))
                return most_rated_language,stop_words_for_language
              </code></pre>
                <hr/>
  <h4> (3) Preprocessing Text</h4>
             <pre><code>        
            def Preprocessing_Text(Raw_text, stop_words_for_language):
                
                # 1 making text as a space seperated word list
                stop_words_for_language = str(stop_words_for_language).lower()
                Words_in_text =[]
                for word in Raw_text.split():                    
                    Words_in_text.append(word)
            
                
                 #2 remove numbers and special charactes from words
                    
                alphawords_only = [word for word in Words_in_text if word.isalpha()]          
                
                #3 removing length 1 words
                
                Words_afterRemoval_onelength = [word for word in alphawords_only if len(word)>1]
            
                #4 lower case all words
                
                lower_case_only = [word.lower() for word in Words_afterRemoval_onelength ]
                
                
                
                stopwords_nltk = set(stopwords.words("English"))  
                words_withoutStopwords = [word for word in lower_case_only if word not in stopwords_nltk]
                if stop_words_for_language != "english":
                    words_withoutStopwords = [word for word in words_withoutStopwords if word not in stop_words_for_language]
                
               
                
                words_withoutCommonNouns = [word for word in words_withoutStopwords if word not in Common_Nouns ]
                
                
                
                return (words_withoutCommonNouns)
            
          
            def Calc_words_frequency(Text_words):
                
                Sorted_WordCount_dict ={}  
                word_and_fr_list=[]
                Count_fr = Counter(Text_words)    
                
                for word,word_count in Count_fr.most_common():
                    word_and_fr_list.append([word, word_count])
                    Sorted_WordCount_dict[word]= word_count
                    
                return(Sorted_WordCount_dict)
              </code></pre>
                <hr/>
    <h4> (4) Feature Formation</h4>
    <pre><code>
            def Function_ParseURL(URL):
                URL =str(URL)
                host=[]
                obj=urlparse(URL)    
                name =(obj.hostname)
                if len(name)>0:
                    for x in name.split('.'):
                        if x.lower() not in URL_CommonQueryWords:
                            host.append(x)
                    else:
                        host.append(name)
                path=[]
                host_part_URL =[]
                      
                for url_parts in URL.split('/'):
                    for url_part in url_parts.split('.'):            
                        if (len(url_part)>0):
                            for url_words in url_part.split('-'):
                                if url_words.lower() not in URL_CommnWords and url_words.lower() not in host: 
                                    path.append(url_words.lower())
                        else:
                            path.append(url_parts)                
                return(host,path)
            
    
            def function_TexDic_Filter(Tag_TextDic):
                alt_words=[]
                if len(Tag_TextDic) > 0:
                    for k,i in Tag_TextDic.items():    
               
                        for x in i:
                            word=[n for n in x.split(',')]
                            for x in word:
                                words=[i for i in x.split() ]
                                for x in words:
                                    alt_words.append(x)
                    return(alt_words)
                else:
                    return(alt_words)
                
            def function_Tag_Text(Raw_HTML_Soup,Tag_name):
                TagTextList=[]  
                for text in Raw_HTML_Soup.find_all(Tag_name):
                    tag_text = text.text.strip().lower()
                    TagTextList.append(tag_text)
                return TagTextList   
            
            def function_HeaderTitleAnchorText(Raw_HTML_Soup):    
                H1_TextList = function_Tag_Text(Raw_HTML_Soup,'h1')
                H2_TextList = function_Tag_Text(Raw_HTML_Soup,'h2')
                H3_TextList= function_Tag_Text(Raw_HTML_Soup,'h3')
                H4_TextList = function_Tag_Text(Raw_HTML_Soup,'h4')
                H5_TextList = function_Tag_Text(Raw_HTML_Soup,'h5')
                H6_TextList = function_Tag_Text(Raw_HTML_Soup,'h6')
                Title_TextList = function_Tag_Text(Raw_HTML_Soup,'title')
                Anchor_TextList = function_Tag_Text(Raw_HTML_Soup,'a')
                return (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,H6_TextList,Title_TextList,Anchor_TextList)
                
                
            def function_MakeDictTagText(Raw_HTML_Soup):
                 
                (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,H6_TextList,Title_TextList,Anchor_TextList) = function_HeaderTitleAnchorText(Raw_HTML_Soup)
                    
                H1_TextDict = {}
                H2_TextDict = {}
                H1_TextDict = {}
                H3_TextDict = {}
                H4_TextDict = {}
                H5_TextDict = {}
                H6_TextDict= {}
                Title_TextDict = {}
                Anchor_TextDict = {}
                    
                H1_TextDict["h1"] = H1_TextList
                H2_TextDict["h2"] = H2_TextList
                H3_TextDict["h3"] = H3_TextList
                H4_TextDict["h4"] = H4_TextList
                H5_TextDict["h5"] = H5_TextList
                H6_TextDict["h6"] = H6_TextList    
                Title_TextDict["title"] = Title_TextList
                Anchor_TextDict["a"] = Anchor_TextList
                
                H1_dic = function_TexDic_Filter(H1_TextDict)
                H2_dic = function_TexDic_Filter(H2_TextDict)
                H3_dic = function_TexDic_Filter(H3_TextDict)
                H4_dic = function_TexDic_Filter(H4_TextDict)
                H5_dic = function_TexDic_Filter(H5_TextDict)
                H6_dic = function_TexDic_Filter(H6_TextDict)
                Title_dic = function_TexDic_Filter(Title_TextDict)
                Anchor_dic = function_TexDic_Filter(Anchor_TextDict)
                
                return (H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic)
              </code></pre>
                <hr/>
    <h4>(5) Score Feature Words</h4>
    <pre><code>
            def Feature_Score(candidate_word,feature_words,score):
                total_score=0
                score_single_time =0    
                for word_feature in feature_words:        
                    if word_feature ==candidate_word:            
                        #total_score+=score
                        score_single_time = score                
                return(score_single_time)
                       
            def Tf_Score(fr,text_length):
                if text_length < 50:
                    tf_score =((fr/100)*50)
                else:
                    tf_score=((fr/100)*20) 
                return (tf_score)   
    
      
            def function_word_Fr_TagName_ScoreDic(words_count_dic, text_length):
                wrd_fr_Tgs_Fnl_score =defaultdict()
                Word_Final_Score =defaultdict()
                
                #names of features 10
                Name_FeaturesList =np.array(['H1', 'H2', 'H3','H4', 'H5', 'H6','Title','Anchor','URL-H','URL-Q'])
                
                # Manual score for words
                Manual_Score_Each_Features =np.array([6, 5, 4,3, 2, 2, 6, 1,5,4])
                
                
                
                # Get all the words in features
                
                (H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic)= function_MakeDictTagText(Raw_HTML_Soup)
                featuresText_allDict_npArrayList = np.array([H1_dic, H2_dic, H3_dic, H4_dic, H5_dic, H6_dic, Title_dic, Anchor_dic, Host_part_of_URL, Query_part_of_URL])
               
                
                for word,fr in words_count_dic.items():
                    tf_score = Tf_Score(fr,text_length)
                    tag =[]
                    name_tag =[]
                           
                    for word_inAll_Dic in range (len(featuresText_allDict_npArrayList)):
                        if word in featuresText_allDict_npArrayList[word_inAll_Dic]:   
                            tag.append(Manual_Score_Each_Features[word_inAll_Dic]) 
                            name_tag.append(Name_FeaturesList[word_inAll_Dic])
                    score= (sum(tag))
                    score = score + tf_score
                    Word_Final_Score[word] = score
                    wrd_fr_Tgs_Fnl_score[word] = fr,name_tag,score
                return (wrd_fr_Tgs_Fnl_score, Word_Final_Score)
              </code></pre>
                <hr/>
    <h4>(6) Final Keyword Selection</h4>
   <pre><code>
            
            if __name__ == "__main__":
                
                URL ="http://bbc.com"
                
                (Raw_text, Raw_HTML_Soup) = Web_Funtion(URL)
                most_rated_language,stop_words_for_language = detect_language(Raw_text)
                
                preprocess_TextWords = Preprocessing_Text(Raw_text, stop_words_for_language )
                text_length = len(preprocess_TextWords)
                words_count_dic = Calc_words_frequency(preprocess_TextWords)
                
                
                # Features
                Host_part_of_URL, Query_part_of_URL = Function_ParseURL(URL)
                
                (H1_TextList,H2_TextList,H3_TextList,H4_TextList,H5_TextList,H6_TextList,Title_TextList,Anchor_TextList) = function_HeaderTitleAnchorText(Raw_HTML_Soup)
               
                #Feature Header, Title, Anchor text, score dictionary
                (wrd_fr_Tgs_Fnl_score, Word_Final_Score) = function_word_Fr_TagName_ScoreDic(words_count_dic, text_length)   
                
            
                #Number of total features 10   
            
                keyword =[]
                sorted_word_score = Counter(Word_Final_Score)
                for word,score in sorted_word_score.most_common(10):
                    keyword.append(word)
                print (keyword)</code></pre>
              <hr/>
              <h4>End Drank Python Implementation</h4>
              <hr/>
      </div>
      
      <div class="output">
        <h4 class = "introduction">Output Section Drank with Example </h4> 
        <div class="result">
          <h5></h5>
          <pre></pre>
          <h5></h5>
          <pre></pre>
        </div>
      </div>



    </div>
  </div>
</section>

<section id="Webrank">
  <div class="container">
    <div class="content">

      <div class="title">
        <h2>WEBRANK </h2>       
      </div>
      <div class = "intro">
      <h4  class = "introduction">Introduction</h4> 
      <article><p></p></article>
        </div>
        
        <div class="methodology">

        <h4 class = "introduction">Methodology</h4> 
        <article><p></p></article>
      </div>
      
        <div class="implementation">
        <h4 class = "introduction">Python implementaion</h4> 
        <pre><code></code></pre>
      </div>
     
      <div class="output">
        <h4 class = "introduction">Output Section Hrank Exmple </h4> 
        <div class="result">
          <h5></h5>
          <pre></pre>
          <h5></h5>
          <pre></pre>
        </div>
      </div>



    </div>
  </div>
</section>

<section id="Acirank">
  <div class="container">
    <div class="content">

      <div class="title">
        <h2>ACI-RANK </h2>       
      </div>
      <div class = "intro">
      <h4  class = "introduction">Introduction</h4> 
      <article><p></p></article>
        </div>
       
        <div class="methodology">

        <h4 class = "introduction">Methodology</h4> 
        <article><p></p></article>
      </div>
      
        <div class="implementation">
        <h4 class = "introduction">Python implementaion</h4> 
        <pre><code></code></pre>
      </div>
      
      <div class="output">
        <h4 class = "introduction">Output Section Hrank Exmple </h4> 
        <div class="result">
          <h5></h5>
          <pre></pre>
          <h5></h5>
          <pre></pre>
        </div>
      </div>



    </div>
  </div>
</section>



   
    
    </main>
    <script src ="app.js"></script>
  </body>
</html>
